# Beamforming-LLM: Semantic Recall of Missed Conversations

**Beamforming-LLM** is a research prototype that enables users to semantically recall conversations they may have missed in multi-speaker environments. It combines spatial audio processing using beamforming with Whisper ASR and Retrieval-Augmented Generation (RAG) via large language models (LLMs). Users can query the system in natural language to understand what was said in overlapping conversations.

### üßæ What the System Outputs

The system produces:
- ‚úÖ **Contrastive natural language summaries** of attended vs. missed conversations
- üìç **Direction metadata** (e.g., left, right, front) indicating spatial origin of speech
- ‚è±Ô∏è **Timestamps** for each conversation segment
- üîä **Audio playback snippets** for both target and non-target streams

This enables intuitive memory-like re-engagement with multi-party conversations, making it useful for meetings, social settings, and assistive recall applications.

---

## üß† Project Overview

In real-world social or professional settings‚Äîsuch as dinner tables, meetings, or poster sessions‚Äîwe often focus on one conversation and miss others happening simultaneously. Beamforming-LLM captures and separates these spatial conversations and enables natural language recall of what was missed.

Key technologies:
- **Beamforming** for directional audio separation
- **Whisper ASR** for transcription and timestamping
- **LLM-RAG** for semantic retrieval and summarization (via GPT-4o-mini)
- **FAISS** for fast vector-based retrieval

---

## üìÅ Notebooks

### `1. Beamforming.ipynb`
Fits beamforming filters and separates multi-channel audio into spatially distinct `.wav` files using:
- MVDR beamforming
- Direction of Arrival (DOA) estimation via Pyroomacoustics

### `2. Beamforming-LLM.ipynb`
Integrates the full pipeline:
- Transcribes separated audio with Whisper
- Chunks and embeds text with sentence transformers
- Stores embeddings in FAISS
- Accepts natural language queries
- Retrieves and filters relevant segments using GPT-4o-mini
- Summarizes what was missed based on temporal-spatial alignment

### `3. Evaluation.ipynb`
Quantitatively evaluates the effectiveness of beamforming using:
- PESQ (Perceptual Evaluation of Speech Quality)
- STOI (Short-Time Objective Intelligibility)
- Comparison of scores before and after beamforming

---
